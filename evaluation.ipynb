{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f79da760",
   "metadata": {},
   "source": [
    "In meinem Modell ist es sehr aussagekräftig ob das Spiel auf neutralem Boden ist oder nicht. Denn wenn das Spiel auf nicht neutralem Boden ist, ist die Chance auf einen Sieg der Heimmanschaft deutlich höher. Beim Ändern des Turniers des Turniers ist der Unterschied nicht so gross."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9656018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DatasetFootball = pd.read_csv(\"Datensatz.csv\", on_bad_lines=\"skip\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "415cc12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DatasetFootball[[\"neutral\", \"tournament\"]]\n",
    "y = DatasetFootball[[\"resultHome\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2c73180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14810, 3703)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "   X, y, test_size = 0.2, random_state = 42\n",
    ")\n",
    "\n",
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "404eeace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors 0.28\n",
      "Stochastic Gradient Descent 0.26\n",
      "Linear SVM 0.46\n",
      "Decision Tree 0.46\n",
      "Random Forest 0.46\n",
      "Neural Net 0.46\n",
      "Naive Bayes 0.45\n",
      "LDA 0.46\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "algorithms = {\n",
    "    \"Nearest Neighbors\" : KNeighborsClassifier(3),\n",
    "    \"Stochastic Gradient Descent\" : SGDClassifier(),\n",
    "    \"Linear SVM\" : SVC(kernel=\"linear\", C=0.025),\n",
    "    \"Decision Tree\" : DecisionTreeClassifier(max_depth=5),\n",
    "    \"Random Forest\" : RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    \"Neural Net\" : MLPClassifier(alpha=1, max_iter=1000),\n",
    "    \"Naive Bayes\" : GaussianNB(),\n",
    "    \"LDA\" : LinearDiscriminantAnalysis(),\n",
    "}\n",
    "\n",
    "\n",
    "for name, algorithm in algorithms.items():\n",
    "\n",
    "    algorithm.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    score = algorithm.score(X_test, y_test)\n",
    "    print(name, round(score,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d01f808f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(algorithms[\"Decision Tree\"], 'DatasetFootball_decision_Tree.joblib')\n",
    "best_model = joblib.load('DatasetFootball_decision_Tree.joblib')\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20789606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timoh\\AppData\\Local\\Temp\\ipykernel_16156\\3630298034.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  best_model.fit(X_train, y_train)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "best_model = RandomForestRegressor(max_depth=45, random_state=0)\n",
    "best_model.fit(X_train, y_train)\n",
    "pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5c00f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "logreg = DecisionTreeClassifier()\n",
    "\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4b7d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f0b3bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4642182014582771\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "198aa12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  18    0 1021]\n",
      " [  15    0  930]\n",
      " [  18    0 1701]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b0ab795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  18    0 1021]\n",
      " [  15    0  930]\n",
      " [  18    0 1701]]\n"
     ]
    }
   ],
   "source": [
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "print(confusion)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ebc9734d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4642182014582771\n"
     ]
    }
   ],
   "source": [
    "##Messmetrik\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995a600",
   "metadata": {},
   "source": [
    "Hier bei der Aufgabe 4.3 hat nicht alles funktioniert, leider weiss ich nicht wieso. Der Code erschien mir richtig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "332b61a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timoh\\AppData\\Local\\Temp\\ipykernel_16156\\3046775800.py:2: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  precision = TP/(TP + FP)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = TP/(TP + FN)\n",
    "precision = TP/(TP + FP)\n",
    "f1 = 2 * ((recall * precision)/(recall + precision))\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "330e2130",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m sensitivität \u001b[38;5;241m=\u001b[39m TP \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(FN \u001b[38;5;241m+\u001b[39m TP)\n\u001b[0;32m      2\u001b[0m (sensitivität)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecall_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_class\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1901\u001b[0m, in \u001b[0;36mrecall_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1770\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecall_score\u001b[39m(\n\u001b[0;32m   1771\u001b[0m     y_true,\n\u001b[0;32m   1772\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1778\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1779\u001b[0m ):\n\u001b[0;32m   1780\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute the recall.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \n\u001b[0;32m   1782\u001b[0m \u001b[38;5;124;03m    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;124;03m    array([1. , 1. , 0.5])\u001b[39;00m\n\u001b[0;32m   1900\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1901\u001b[0m     _, r, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1902\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1903\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1906\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1908\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1910\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1911\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1544\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1543\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1544\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1547\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1365\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1364\u001b[0m             average_options\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1365\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1366\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget is \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m but average=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1367\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoose another average setting, one of \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (y_type, average_options)\n\u001b[0;32m   1368\u001b[0m         )\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pos_label \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1370\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that pos_label (set to \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) is ignored when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage != \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m). You may use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1375\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1376\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "sensitivität = TP / float(FN + TP)\n",
    "(sensitivität)\n",
    "print(metrics.recall_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27c791a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "spezifität = TN / (TN + FP)\n",
    "print(spezifität)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e992ce",
   "metadata": {},
   "source": [
    "4.4\n",
    "Mein Modell aus Aufgabe 3 funktioniert zu etwa 50% das heisst etwa jedes zweite Mal die Ausgabe richtig ausgibt. Ich denke der Parameter ist am wichtigsten/aussagekräftigsten. Wenn man den Parameter neutral auf wahr setzt ist mein Algorithmus ungenauer, als wenn er auch nicht neutral gesetzt ist. Bei den Spielen auf nicht neutralem Boden gewinnt nämlich deutlich öfters die Heimmannschaft, wie es mein Algorithmus auch ausgibt. Bei Spielen auf neutralem Boden ist es aber ziemlich ausgeglichen. Also ist die Vorhersage genauer, wenn man bei neutral 1(nicht neutral) als Parameter mitgibt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
